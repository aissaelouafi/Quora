{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quora Kaggle competition\n",
    "\n",
    "Welcome to the Quora Question Pairs competition! Here, our goal is to identify which questions asked on Quora, a quasi-forum website with over 100 million visitors a month, are duplicates of questions that have already been asked. This could be useful, for example, to instantly provide answers to questions that have already been answered. We are tasked with predicting whether a pair of questions are duplicates or not, and submitting a binary prediction against the logloss metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os,re\n",
    "import cPickle\n",
    "import seaborn as sns\n",
    "import gensim as gn\n",
    "import logging\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import nltk\n",
    "import scipy.sparse as sparse\n",
    "from nltk.data import load\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn import linear_model\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "import pyLDAvis\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from pyemd import emd\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "%matplotlib inline\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('./data/train.csv',nrows=1000)\n",
    "df_test = pd.read_csv('./data/test.csv',nrows=1000)\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of question pairs for training: 1000\n",
      "Total number of question pairs for test data: 1000\n",
      "Duplicate pairs : 38.0 %\n"
     ]
    }
   ],
   "source": [
    "print('Total number of question pairs for training: {}'.format(len(df_train)))\n",
    "print('Total number of question pairs for test data: {}'.format(len(df_test)))\n",
    "print('Duplicate pairs : {} %'.format(round(df_train['is_duplicate'].mean()*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text handcrafted features (fs_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_features(df_train):\n",
    "    df_train['len_q1'] = df_train['question1'].apply(lambda x:len(str(x)))\n",
    "    df_train['len_q2'] = df_train['question2'].apply(lambda x:len(str(x)))\n",
    "    df_train['diff_len'] = df_train.len_q1-df_train.len_q2\n",
    "    df_train['len_char_q1'] = df_train.question1.apply(lambda x:len(''.join(set(str(x).replace(' ','')))))\n",
    "    df_train['len_char_q2'] = df_train.question2.apply(lambda x:len(''.join(set(str(x).replace(' ','')))))\n",
    "    df_train['len_word_q1'] = df_train.question1.apply(lambda x:len(str(x).split()))\n",
    "    df_train['len_word_q2'] = df_train.question2.apply(lambda x:len(str(x).split()))\n",
    "    df_train['common_words'] = df_train.apply(lambda x:len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))),axis=1)\n",
    "\n",
    "    df_train['fuzzy_qratio'] = df_train.apply(lambda x: fuzz.QRatio(str(x['question1']),str(x['question2'])),axis=1)\n",
    "    df_train['fuzzy_wratio'] = df_train.apply(lambda x:fuzz.WRatio(str(x['question1']),str(x['question2'])),axis=1)\n",
    "    df_train['fuzzy_partial_ratio'] = df_train.apply(lambda x:fuzz.partial_ratio(str(x['question1']),str(x['question2'])),axis=1)\n",
    "    df_train['fuzzy_partial_token_set_ratio'] = df_train.apply(lambda x:fuzz.partial_token_set_ratio(str(x['question1']),str(x['question2'])),axis=1)\n",
    "    df_train['fuzzy_partial_token_sort_ratio'] = df_train.apply(lambda x:fuzz.partial_token_sort_ratio(str(x['question1']),str(x['question2'])),axis=1)\n",
    "    df_train['fuzzy_token_sort_ratio'] = df_train.apply(lambda x:fuzz.token_sort_ratio(str(x['question1']),str(x['question2'])),axis=1)\n",
    "    df_train['fuzzy_token_set_ratio'] = df_train.apply(lambda x:fuzz.token_set_ratio(str(x['question1']),str(x['question2'])),axis=1)\n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>len_q1</th>\n",
       "      <th>len_q2</th>\n",
       "      <th>diff_len</th>\n",
       "      <th>len_char_q1</th>\n",
       "      <th>len_char_q2</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>common_words</th>\n",
       "      <th>fuzzy_qratio</th>\n",
       "      <th>fuzzy_wratio</th>\n",
       "      <th>fuzzy_partial_ratio</th>\n",
       "      <th>fuzzy_partial_token_set_ratio</th>\n",
       "      <th>fuzzy_partial_token_sort_ratio</th>\n",
       "      <th>fuzzy_token_sort_ratio</th>\n",
       "      <th>fuzzy_token_set_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>57</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>93</td>\n",
       "      <td>95</td>\n",
       "      <td>98</td>\n",
       "      <td>100</td>\n",
       "      <td>88</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>88</td>\n",
       "      <td>-37</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>66</td>\n",
       "      <td>86</td>\n",
       "      <td>73</td>\n",
       "      <td>100</td>\n",
       "      <td>73</td>\n",
       "      <td>63</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>59</td>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>43</td>\n",
       "      <td>60</td>\n",
       "      <td>41</td>\n",
       "      <td>100</td>\n",
       "      <td>71</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "\n",
       "                                           question2  is_duplicate  len_q1  \\\n",
       "0  What is the step by step guide to invest in sh...             0      66   \n",
       "1  What would happen if the Indian government sto...             0      51   \n",
       "2  How can Internet speed be increased by hacking...             0      73   \n",
       "\n",
       "   len_q2  diff_len  len_char_q1  len_char_q2  len_word_q1  len_word_q2  \\\n",
       "0      57         9           20           20           14           12   \n",
       "1      88       -37           21           29            8           13   \n",
       "2      59        14           25           24           14           10   \n",
       "\n",
       "   common_words  fuzzy_qratio  fuzzy_wratio  fuzzy_partial_ratio  \\\n",
       "0            10            93            95                   98   \n",
       "1             4            66            86                   73   \n",
       "2             4            43            60                   41   \n",
       "\n",
       "   fuzzy_partial_token_set_ratio  fuzzy_partial_token_sort_ratio  \\\n",
       "0                            100                              88   \n",
       "1                            100                              73   \n",
       "2                            100                              71   \n",
       "\n",
       "   fuzzy_token_sort_ratio  fuzzy_token_set_ratio  \n",
       "0                      93                    100  \n",
       "1                      63                     86  \n",
       "2                      63                     63  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = generate_features(df_train)\n",
    "df_test = generate_features(df_test)\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA (Lattent Dirichlet Allocation) features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Steaming \n",
    "p_stemmer = PorterStemmer()\n",
    "STOP_WORDS = nltk.corpus.stopwords.words()\n",
    "\n",
    "def clean_sentence(val):\n",
    "    \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n",
    "    regex = re.compile('([^\\s\\w]|_)+')\n",
    "    sentence = regex.sub('', val).lower()\n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    for word in list(sentence):\n",
    "        if word in STOP_WORDS:\n",
    "            sentence.remove(word)  \n",
    "            \n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence\n",
    "\n",
    "def clean_dataframe(data):\n",
    "    \"drop nans, then apply 'clean_sentence' function to question1 and 2\"\n",
    "    data = data.dropna(how=\"any\")\n",
    "    \n",
    "    for col in ['question1', 'question2']:\n",
    "        data[col] = data[col].apply(clean_sentence)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Function to vuild a corpus\n",
    "def build_corpus(data):\n",
    "    \"Creates a list of lists containing words from each sentence\"\n",
    "    corpus = []\n",
    "    for col in ['question1', 'question2']:\n",
    "        for sentence in data[col].iteritems():\n",
    "            word_list = sentence[1].split(\" \")\n",
    "            corpus.append(word_list)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = clean_dataframe(df_train)\n",
    "corpus = build_corpus(data)\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "corpus = [dictionary.doc2bow(text) for text in corpus]\n",
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=100, id2word = dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def common_lda_topic(sentence1,sentence2,dictionary,ldamodel,min_proba):\n",
    "    \"find #common topic based on lattent dirichlet allocation model\"\n",
    "    sentence1 = sentence1.split()\n",
    "    sentence2 = sentence2.split()\n",
    "\n",
    "    sentence1 = dictionary.doc2bow(sentence1)\n",
    "    sentence2 = dictionary.doc2bow(sentence2)\n",
    "    \n",
    "    topic_a = ldamodel.get_document_topics(sentence1,minimum_probability=min_proba)\n",
    "    topic_b = ldamodel.get_document_topics(sentence2,minimum_probability=min_proba)\n",
    "    \n",
    "    topic_a = list(sorted(topic_a, key=lambda x: x[1]))\n",
    "    topic_b = list(sorted(topic_b, key=lambda x: x[1]))\n",
    "    common_topic = set([x[0] for x in topic_a]).intersection(x[0] for x in topic_b)\n",
    "    return(len(common_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vis_data = gensimvis.prepare(ldamodel, corpus, dictionary)\n",
    "#pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train['common_topics'] = df_train.apply(lambda x:common_lda_topic(str(x['question1']),str(x['question2']),dictionary,ldamodel,0.1),axis=1)\n",
    "df_test['common_topics'] = df_test.apply(lambda x:common_lda_topic(str(x['question1']),str(x['question2']),dictionary,ldamodel,0.1),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS-Tagging features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def common_pos_tagging(question1,question2):\n",
    "    question1 = nltk.word_tokenize(question1)\n",
    "    question2 = nltk.word_tokenize(question2)\n",
    "    pos_question1 = nltk.pos_tag(question1)\n",
    "    pos_question2 = nltk.pos_tag(question2)\n",
    "\n",
    "    pos_1_array = [x[1] for x in pos_question1]\n",
    "    pos_2_array = [x[1] for x in pos_question2]\n",
    "    return(len(set(pos_1_array).intersection(pos_2_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_distinct_pos_tagging(df_train):\n",
    "    #Generate all pos-tag null columns\n",
    "    tagdict = load('help/tagsets/upenn_tagset.pickle')\n",
    "    pos_tag = tagdict.keys()\n",
    "    for tag in pos_tag:\n",
    "        df_train[tag+\"_q1\"] = 0\n",
    "        df_train[tag+\"_q2\"] = 0\n",
    "        \n",
    "    for index, row in df_train.iterrows():\n",
    "        question1 = row.question1.decode('utf-8')\n",
    "        question1 = nltk.word_tokenize(question1)\n",
    "\n",
    "        question2 = row.question2.decode('utf-8')\n",
    "        question2 = nltk.word_tokenize(question2)\n",
    "\n",
    "        pos_question1 = nltk.pos_tag(question1)\n",
    "        pos_question1 = [x[1] for x in pos_question1]\n",
    "\n",
    "        pos_question2 = nltk.pos_tag(question2)\n",
    "        pos_question2 = [x[1] for x in pos_question2]\n",
    "\n",
    "        for tag in pos_question1:\n",
    "            if(tag != \"#\"):\n",
    "                df_train.set_value(index,tag+\"_q1\",row[tag+\"_q1\"]+1)\n",
    "\n",
    "        for tag in pos_question2:\n",
    "            if(tag != \"#\"):\n",
    "                df_train.set_value(index,tag+\"_q2\",row[tag+\"_q2\"]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train['common_pos_count'] = df_train.apply(lambda x:common_pos_tagging(str(x['question1']).decode('utf-8'),str(x['question2']).decode('utf-8')),axis=1)\n",
    "df_test['common_pos_count'] = df_test.apply(lambda x:common_pos_tagging(str(x['question1']).decode('utf-8'),str(x['question2']).decode('utf-8')),axis=1)\n",
    "count_distinct_pos_tagging(df_train)\n",
    "count_distinct_pos_tagging(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>len_q1</th>\n",
       "      <th>len_q2</th>\n",
       "      <th>diff_len</th>\n",
       "      <th>len_char_q1</th>\n",
       "      <th>len_char_q2</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>common_words</th>\n",
       "      <th>fuzzy_qratio</th>\n",
       "      <th>fuzzy_wratio</th>\n",
       "      <th>fuzzy_partial_ratio</th>\n",
       "      <th>fuzzy_partial_token_set_ratio</th>\n",
       "      <th>fuzzy_partial_token_sort_ratio</th>\n",
       "      <th>fuzzy_token_sort_ratio</th>\n",
       "      <th>fuzzy_token_set_ratio</th>\n",
       "      <th>common_topics</th>\n",
       "      <th>common_pos_count</th>\n",
       "      <th>PRP$_q1</th>\n",
       "      <th>PRP$_q2</th>\n",
       "      <th>VBG_q1</th>\n",
       "      <th>VBG_q2</th>\n",
       "      <th>VBD_q1</th>\n",
       "      <th>VBD_q2</th>\n",
       "      <th>``_q1</th>\n",
       "      <th>``_q2</th>\n",
       "      <th>VBN_q1</th>\n",
       "      <th>VBN_q2</th>\n",
       "      <th>,_q1</th>\n",
       "      <th>,_q2</th>\n",
       "      <th>''_q1</th>\n",
       "      <th>''_q2</th>\n",
       "      <th>VBP_q1</th>\n",
       "      <th>VBP_q2</th>\n",
       "      <th>WDT_q1</th>\n",
       "      <th>WDT_q2</th>\n",
       "      <th>JJ_q1</th>\n",
       "      <th>JJ_q2</th>\n",
       "      <th>WP_q1</th>\n",
       "      <th>WP_q2</th>\n",
       "      <th>VBZ_q1</th>\n",
       "      <th>VBZ_q2</th>\n",
       "      <th>DT_q1</th>\n",
       "      <th>DT_q2</th>\n",
       "      <th>RP_q1</th>\n",
       "      <th>RP_q2</th>\n",
       "      <th>$_q1</th>\n",
       "      <th>$_q2</th>\n",
       "      <th>NN_q1</th>\n",
       "      <th>NN_q2</th>\n",
       "      <th>)_q1</th>\n",
       "      <th>)_q2</th>\n",
       "      <th>(_q1</th>\n",
       "      <th>(_q2</th>\n",
       "      <th>FW_q1</th>\n",
       "      <th>FW_q2</th>\n",
       "      <th>POS_q1</th>\n",
       "      <th>POS_q2</th>\n",
       "      <th>._q1</th>\n",
       "      <th>._q2</th>\n",
       "      <th>TO_q1</th>\n",
       "      <th>TO_q2</th>\n",
       "      <th>LS_q1</th>\n",
       "      <th>LS_q2</th>\n",
       "      <th>RB_q1</th>\n",
       "      <th>RB_q2</th>\n",
       "      <th>:_q1</th>\n",
       "      <th>:_q2</th>\n",
       "      <th>NNS_q1</th>\n",
       "      <th>NNS_q2</th>\n",
       "      <th>NNP_q1</th>\n",
       "      <th>NNP_q2</th>\n",
       "      <th>VB_q1</th>\n",
       "      <th>VB_q2</th>\n",
       "      <th>WRB_q1</th>\n",
       "      <th>WRB_q2</th>\n",
       "      <th>CC_q1</th>\n",
       "      <th>CC_q2</th>\n",
       "      <th>PDT_q1</th>\n",
       "      <th>PDT_q2</th>\n",
       "      <th>RBS_q1</th>\n",
       "      <th>RBS_q2</th>\n",
       "      <th>RBR_q1</th>\n",
       "      <th>RBR_q2</th>\n",
       "      <th>CD_q1</th>\n",
       "      <th>CD_q2</th>\n",
       "      <th>PRP_q1</th>\n",
       "      <th>PRP_q2</th>\n",
       "      <th>EX_q1</th>\n",
       "      <th>EX_q2</th>\n",
       "      <th>IN_q1</th>\n",
       "      <th>IN_q2</th>\n",
       "      <th>WP$_q1</th>\n",
       "      <th>WP$_q2</th>\n",
       "      <th>MD_q1</th>\n",
       "      <th>MD_q2</th>\n",
       "      <th>NNPS_q1</th>\n",
       "      <th>NNPS_q2</th>\n",
       "      <th>--_q1</th>\n",
       "      <th>--_q2</th>\n",
       "      <th>JJS_q1</th>\n",
       "      <th>JJS_q2</th>\n",
       "      <th>JJR_q1</th>\n",
       "      <th>JJR_q2</th>\n",
       "      <th>SYM_q1</th>\n",
       "      <th>SYM_q2</th>\n",
       "      <th>UH_q1</th>\n",
       "      <th>UH_q2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>57</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>93</td>\n",
       "      <td>95</td>\n",
       "      <td>98</td>\n",
       "      <td>100</td>\n",
       "      <td>88</td>\n",
       "      <td>93</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>88</td>\n",
       "      <td>-37</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>66</td>\n",
       "      <td>86</td>\n",
       "      <td>73</td>\n",
       "      <td>100</td>\n",
       "      <td>73</td>\n",
       "      <td>63</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>59</td>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>43</td>\n",
       "      <td>60</td>\n",
       "      <td>41</td>\n",
       "      <td>100</td>\n",
       "      <td>71</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "\n",
       "                                           question2  is_duplicate  len_q1  \\\n",
       "0  What is the step by step guide to invest in sh...             0      66   \n",
       "1  What would happen if the Indian government sto...             0      51   \n",
       "2  How can Internet speed be increased by hacking...             0      73   \n",
       "\n",
       "   len_q2  diff_len  len_char_q1  len_char_q2  len_word_q1  len_word_q2  \\\n",
       "0      57         9           20           20           14           12   \n",
       "1      88       -37           21           29            8           13   \n",
       "2      59        14           25           24           14           10   \n",
       "\n",
       "   common_words  fuzzy_qratio  fuzzy_wratio  fuzzy_partial_ratio  \\\n",
       "0            10            93            95                   98   \n",
       "1             4            66            86                   73   \n",
       "2             4            43            60                   41   \n",
       "\n",
       "   fuzzy_partial_token_set_ratio  fuzzy_partial_token_sort_ratio  \\\n",
       "0                            100                              88   \n",
       "1                            100                              73   \n",
       "2                            100                              71   \n",
       "\n",
       "   fuzzy_token_sort_ratio  fuzzy_token_set_ratio  common_topics  \\\n",
       "0                      93                    100              1   \n",
       "1                      63                     86              0   \n",
       "2                      63                     63              0   \n",
       "\n",
       "   common_pos_count  PRP$_q1  PRP$_q2  VBG_q1  VBG_q2  VBD_q1  VBD_q2  ``_q1  \\\n",
       "0                 9        0        0       0       0       0       0      0   \n",
       "1                 8        0        0       0       0       0       1      0   \n",
       "2                 8        1        0       1       1       0       0      0   \n",
       "\n",
       "   ``_q2  VBN_q1  VBN_q2  ,_q1  ,_q2  ''_q1  ''_q2  VBP_q1  VBP_q2  WDT_q1  \\\n",
       "0      0       0       0     0     0      0      0       0       0       0   \n",
       "1      0       0       0     0     0      0      0       0       0       0   \n",
       "2      0       0       1     0     0      0      0       0       0       0   \n",
       "\n",
       "   WDT_q2  JJ_q1  JJ_q2  WP_q1  WP_q2  VBZ_q1  VBZ_q2  DT_q1  DT_q2  RP_q1  \\\n",
       "0       0      0      0      1      1       1       1      1      1      0   \n",
       "1       0      0      1      1      1       1       0      1      1      0   \n",
       "2       0      1      0      0      0       0       0      1      0      0   \n",
       "\n",
       "   RP_q2  $_q1  $_q2  NN_q1  NN_q2  )_q1  )_q2  (_q1  (_q2  FW_q1  FW_q2  \\\n",
       "0      0     0     0      1      1     0     0     0     0      0      0   \n",
       "1      0     0     0      1      1     1     1     1     1      0      0   \n",
       "2      0     0     0      1      1     0     0     0     0      0      0   \n",
       "\n",
       "   POS_q1  POS_q2  ._q1  ._q2  TO_q1  TO_q2  LS_q1  LS_q2  RB_q1  RB_q2  :_q1  \\\n",
       "0       0       0     1     1      1      1      0      0      1      1     0   \n",
       "1       0       0     1     1      0      0      0      0      0      1     0   \n",
       "2       0       0     1     1      0      0      0      0      0      0     0   \n",
       "\n",
       "   :_q2  NNS_q1  NNS_q2  NNP_q1  NNP_q2  VB_q1  VB_q2  WRB_q1  WRB_q2  CC_q1  \\\n",
       "0     0       0       0       0       0      1      1       0       0      0   \n",
       "1     0       0       0       1       1      0      1       0       0      0   \n",
       "2     0       0       0       1       1      1      1       1       1      0   \n",
       "\n",
       "   CC_q2  PDT_q1  PDT_q2  RBS_q1  RBS_q2  RBR_q1  RBR_q2  CD_q1  CD_q2  \\\n",
       "0      0       0       0       0       0       0       0      0      0   \n",
       "1      0       0       0       0       0       0       0      0      0   \n",
       "2      0       0       0       0       0       0       0      0      0   \n",
       "\n",
       "   PRP_q1  PRP_q2  EX_q1  EX_q2  IN_q1  IN_q2  WP$_q1  WP$_q2  MD_q1  MD_q2  \\\n",
       "0       0       0      0      0      1      1       0       0      0      0   \n",
       "1       0       0      0      0      1      1       0       0      0      1   \n",
       "2       1       0      0      0      1      1       0       0      1      1   \n",
       "\n",
       "   NNPS_q1  NNPS_q2  --_q1  --_q2  JJS_q1  JJS_q2  JJR_q1  JJR_q2  SYM_q1  \\\n",
       "0        0        0      0      0       0       0       0       0       0   \n",
       "1        0        0      0      0       0       0       0       0       0   \n",
       "2        0        0      0      0       0       0       0       0       0   \n",
       "\n",
       "   SYM_q2  UH_q1  UH_q2  \n",
       "0       0      0      0  \n",
       "1       0      0      0  \n",
       "2       0      0      0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec features \n",
    "Word2Vec creates a multi-dimensional vector for every word in the english vocabulary (or the corpus it has been trained on). Word2Vec embeddings are very popular in natural language processing and always provide us with great insights. Wikipedia provides a good explanation of what these embeddings are and how they are generated (https://en.wikipedia.org/wiki/Word2vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = gn.models.KeyedVectors.load_word2vec_format('./data/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec mean of president : 0.00630775932223 mean of obama 0.00169826287311\n"
     ]
    }
   ],
   "source": [
    "print(\"Word2vec mean of president : {} mean of obama {}\".format(model['president'].mean(),model['king'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to build a vector of each word in differents sentences (question1, question2), each word will be presented in a 300 dimensions vector. So we will try to calculate the average of these vector in order to compare the result obtained in the first and second question. I will try to calculate differents distances based on theses vectors such as euclidiant distance, cosine similiratity, hamming distance ... and generate some features based on semantic of words. So, if sentence $s1$ contains $K$ words, we can present this sentence by $K$ arrays of $300$ dimensions. So a matrix of $[K*300]$.\n",
    "\n",
    "An example of the application of this model is to find the word the most similar to a particular word (semantic sense I mean). Lets try to find the most similar word to `Obama`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Barack_Obama', 0.8036513328552246),\n",
       " (u'President_Barack_Obama', 0.7878769040107727),\n",
       " (u'McCain', 0.7555227279663086),\n",
       " (u'Clinton', 0.7526832818984985),\n",
       " (u'Illinois_senator', 0.74974524974823),\n",
       " (u'Biden', 0.7485178709030151),\n",
       " (u'Bush', 0.7348896265029907),\n",
       " (u'Barack', 0.7290467023849487),\n",
       " (u'White_House', 0.7151209115982056),\n",
       " (u'elect_Barack_Obama', 0.6941337585449219)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"Obama\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent2vec(s):\n",
    "    words = str(s).lower().decode('utf-8')\n",
    "    words = nltk.word_tokenize(words)\n",
    "    words = [w for w in words if not w in STOP_WORDS]\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(model[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will take now an example of a sentence and apply the word2vec model of words that contains, Lets take the first observation of the train dataframe. It's `'What is the step by step guide to invest in share market in india?'`, so this sentences contains 7 words (after deleting stop words). So this sentence will be presented by a $[7*300]$ matrix. Let's do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentences vectors [[-0.07852414  0.04130891  0.04205321 ...,  0.11536722  0.02307344\n",
      "  -0.00869906]\n",
      " [-0.07852414  0.04130891  0.04205321 ...,  0.11536722  0.02307344\n",
      "  -0.00869906]\n",
      " [ 0.01465341 -0.00877338 -0.0530136  ..., -0.01913343  0.06570699\n",
      "   0.05861362]\n",
      " ..., \n",
      " [-0.02046456 -0.02115659 -0.04092911 ..., -0.03796323 -0.0039545\n",
      "  -0.08660363]\n",
      " [-0.06247045 -0.03513963 -0.09175348 ...,  0.05661384  0.01893635\n",
      "  -0.00541736]\n",
      " [-0.0631949  -0.01935344  0.00284706 ..., -0.02567293 -0.03133414\n",
      "  -0.03028089]], the shape of this vector is : (7, 300)\n"
     ]
    }
   ],
   "source": [
    "m = sent2vec(df_train.iloc[0].question1)\n",
    "print(\"The sentences vectors {}, the shape of this vector is : {}\".format(m,m.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The disadvantage of the `word2vec` model that we can't present all the sentence but only words, so we will try some metodologies to present all the sentence using the `word2vec` model, we will for example try to sum the `word2vec` array and calculate the distance between the 2 questions. So each question will be presented using a ${R}^{300}$ vector, and calculate the distance between two ${R}^{300}$ vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Barack_Obama', 0.8036513328552246),\n",
       " (u'President_Barack_Obama', 0.7878769040107727),\n",
       " (u'McCain', 0.7555227279663086),\n",
       " (u'Clinton', 0.7526832818984985),\n",
       " (u'Illinois_senator', 0.74974524974823),\n",
       " (u'Biden', 0.7485178709030151),\n",
       " (u'Bush', 0.7348896265029907),\n",
       " (u'Barack', 0.7290467023849487),\n",
       " (u'White_House', 0.7151209115982056),\n",
       " (u'elect_Barack_Obama', 0.6941337585449219)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=[\"Obama\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Word mover's distance feature : \n",
    "In document classification and other natural language processing applications, having a good measure of the similarity of two texts can be a valuable building block. Ideally, such a measure would capture semantic information. Cosine similarity on bag-of-words vectors is known to do well in practice, but it inherently cannot capture when documents say the same thing in completely different words. Take, for example, two headlines:\n",
    "\n",
    "<b>`- Obama speaks to the media in Illinois`</b>\n",
    "\n",
    "<b>`- The President greets the press in Chicago`</b>\n",
    "\n",
    "These have no content words in common, so according to most bag of words—based metrics, their distance would be maximal. (For such applications, you probably don’t want to count stopwords such as the and in, which don’t truly signal semantic similarity.)\n",
    "\n",
    "\n",
    "The distance between two texts is given by the total amount of “mass” needed to move the words from one side into the other, multiplied by the distance the words need to move. So, starting from a measure of the distance between different words, we can get a principled document-level distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wmd(s1,s2):\n",
    "    s1 = str(s1).lower().decode('utf-8').split()\n",
    "    s2 = str(s2).lower().decode('utf-8').split()\n",
    "    s1 = [w for w in s1 if w not in STOP_WORDS]\n",
    "    s2 = [w for w in s2 if w not in STOP_WORDS]\n",
    "    return model.wmdistance(s1,s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train['wmd'] = df_train.apply(lambda x:model.wmdistance(x['question1'],x['question2']),axis=1)\n",
    "df_test['wmd'] = df_test.apply(lambda x:model.wmdistance(x['question1'],x['question2']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1000.000000\n",
       "mean        0.268004\n",
       "std         0.125539\n",
       "min         0.000000\n",
       "25%         0.186729\n",
       "50%         0.257678\n",
       "75%         0.330898\n",
       "max         1.105248\n",
       "Name: wmd, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.wmd.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculcate differents distances (euclidian, cosine, jaccard, monkowski ...)\n",
    "We will try now to calculate the distance between two questions. i.e between the two ${R}^{300}$ vector, so we will present these two vector as 2 points in a plan of $300$ dimensions, and calculate the distance between those points. Let's do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word2vec_sentences(s):\n",
    "    M = sent2vec(s)\n",
    "    v = M.sum(axis=0)\n",
    "    return v / np.sqrt((v ** 2).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "159it [00:00, 1581.79it/s]/Users/aissaelouafi/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "1000it [00:00, 1736.90it/s]\n",
      "1000it [00:00, 1778.51it/s]\n",
      "1000it [00:00, 1723.37it/s]\n",
      "1000it [00:00, 1683.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# Array contains the word2vec sum (sentences)\n",
    "question1_vectors = np.zeros((df_train.shape[0], 300))\n",
    "question2_vectors = np.zeros((df_train.shape[0], 300))\n",
    "\n",
    "question1_test_vectors = np.zeros((df_train.shape[0], 300))\n",
    "question2_test_vectors = np.zeros((df_train.shape[0], 300))\n",
    "for i, q in tqdm(enumerate(df_train.question1.values)):\n",
    "    question1_vectors[i, :] = word2vec_sentences(q)\n",
    "\n",
    "for i,q in tqdm(enumerate(df_train.question2.values)):\n",
    "    question2_vectors[i, :] = word2vec_sentences(q)\n",
    "    \n",
    "for i, q in tqdm(enumerate(df_test.question1.values)):\n",
    "    question1_test_vectors[i, :] = word2vec_sentences(q)\n",
    "\n",
    "for i,q in tqdm(enumerate(df_test.question2.values)):\n",
    "    question2_test_vectors[i, :] = word2vec_sentences(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),np.nan_to_num(question2_vectors))]\n",
    "df_train['euclidean_distance'] = [euclidean(x, y) for(x, y) in zip(np.nan_to_num(question1_vectors),np.nan_to_num(question2_vectors))]\n",
    "\n",
    "df_test['cosine_distance'] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_test_vectors),np.nan_to_num(question2_test_vectors))]\n",
    "df_test['euclidean_distance'] = [euclidean(x, y) for(x, y) in zip(np.nan_to_num(question1_test_vectors),np.nan_to_num(question2_test_vectors))]\n",
    "\n",
    "\n",
    "df_train['jaccard_distance'] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),np.nan_to_num(question2_vectors))]\n",
    "df_test['jaccard_distance'] = [jaccard(x, y) for(x, y) in zip(np.nan_to_num(question1_test_vectors),np.nan_to_num(question2_test_vectors))]\n",
    "\n",
    "\n",
    "df_train['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),np.nan_to_num(question2_vectors))]\n",
    "df_test['canberra_distance'] = [canberra(x, y) for(x, y) in zip(np.nan_to_num(question1_test_vectors),np.nan_to_num(question2_test_vectors))]\n",
    "\n",
    "\n",
    "df_train['minkowski_distance'] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors),np.nan_to_num(question2_vectors))]\n",
    "df_test['minkowski_distance'] = [minkowski(x, y, 3) for(x, y) in zip(np.nan_to_num(question1_test_vectors),np.nan_to_num(question2_test_vectors))]\n",
    "\n",
    "\n",
    "df_train['braycurtis_distance'] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),np.nan_to_num(question2_vectors))]\n",
    "df_test['braycurtis_distance'] = [braycurtis(x, y) for(x, y) in zip(np.nan_to_num(question1_test_vectors),np.nan_to_num(question2_test_vectors))]\n",
    "\n",
    "\n",
    "df_train['canberra_distance'] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),np.nan_to_num(question2_vectors))]\n",
    "df_test['canberra_distance'] = [canberra(x, y) for(x, y) in zip(np.nan_to_num(question1_test_vectors),np.nan_to_num(question2_test_vectors))]\n",
    "\n",
    "\n",
    "df_train['cityblock_distance'] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),np.nan_to_num(question2_vectors))]\n",
    "df_test['cityblock_distance'] = [cityblock(x, y) for(x, y) in zip(np.nan_to_num(question1_test_vectors),np.nan_to_num(question2_test_vectors))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1000.000000\n",
       "mean        0.671730\n",
       "std         0.317851\n",
       "min         0.000000\n",
       "25%         0.494646\n",
       "50%         0.687001\n",
       "75%         0.876886\n",
       "max         1.362603\n",
       "Name: euclidean_distance, dtype: float64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.euclidean_distance.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features important with ExtraTreesClassifier\n",
    "LDA is a supervised stastical technique to reduce dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJcAAALXCAYAAAAwgaG+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20864dbd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build a forest and compute the feature importances\n",
    "forest = ExtraTreesClassifier(n_estimators=250,random_state=0)\n",
    "train = df_train.ix[:, 6:,]\n",
    "train = train.fillna(0)\n",
    "forest.fit(train, df_train.is_duplicate)\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "list = train.columns.values\n",
    "# Print the feature ranking\n",
    "#print(\"Feature ranking:\")\n",
    "\n",
    "#for f in range(train.shape[1]):\n",
    "#    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(train.shape[1]), importances[indices],color='#ffdcad', yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(train.shape[1]), list[indices],rotation='vertical')\n",
    "plt.xlim([-1, 25])\n",
    "plt.ylim([0,.1])\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Feature label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "X = df_train.ix[:, 6:,]\n",
    "X = X.fillna(0)\n",
    "Y = df_train.is_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape : (1000, 115), Test shape : (1000, 115)\n"
     ]
    }
   ],
   "source": [
    "print('Train shape : {}, Test shape : {}'.format(X.shape,df_test.ix[:, 3:,].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_test = df_test.fillna(0)\n",
    "proba_replicated = logreg.predict_proba(df_test.ix[:, 3:,])\n",
    "proba = proba_replicated[:,1]\n",
    "len(proba)\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = df_test['test_id']\n",
    "sub['is_duplicate'] = proba\n",
    "sub.to_csv('simple_lr.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log loss cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67599999999999993"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(logreg, X, Y, cv=5, scoring='accuracy')\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-ac6cebb5a411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# evaluate predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Set our parameters for xgboost\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "params['eval_metric'] = 'logloss'\n",
    "params['eta'] = 0.02\n",
    "params['max_depth'] = 4\n",
    "\n",
    "x_train = df_train.ix[:, 6:,]\n",
    "y_train = df_train.is_duplicate\n",
    "\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "\n",
    "\n",
    "bst = xgb.train(params, d_train, 400)\n",
    "\n",
    "test = df_test.ix[:, 3:,]\n",
    "d_test = xgb.DMatrix(test)\n",
    "p_test = bst.predict(d_test)\n",
    "\n",
    "predictions = [round(value) for value in p_test]\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"And now for something completely different\")\n",
    "nltk.pos_tag(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
