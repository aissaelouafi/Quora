{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quora Kaggle competition\n",
    "\n",
    "Welcome to the Quora Question Pairs competition! Here, our goal is to identify which questions asked on Quora, a quasi-forum website with over 100 million visitors a month, are duplicates of questions that have already been asked. This could be useful, for example, to instantly provide answers to questions that have already been answered. We are tasked with predicting whether a pair of questions are duplicates or not, and submitting a binary prediction against the logloss metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os,re\n",
    "import seaborn as sns\n",
    "import gensim as gn\n",
    "import logging\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import nltk\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn import linear_model\n",
    "import keras.layers as lyr\n",
    "from keras.models import Model\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>273872</th>\n",
       "      <td>273872</td>\n",
       "      <td>392405</td>\n",
       "      <td>392406</td>\n",
       "      <td>What are the pros &amp; cons of democracy?</td>\n",
       "      <td>What are the pros and cons of a democracy?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342308</th>\n",
       "      <td>342308</td>\n",
       "      <td>438846</td>\n",
       "      <td>462606</td>\n",
       "      <td>How will Brexit impact the flow of goods and p...</td>\n",
       "      <td>Can a post-Brexit Britain really survive witho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353135</th>\n",
       "      <td>353135</td>\n",
       "      <td>482139</td>\n",
       "      <td>482140</td>\n",
       "      <td>If I got a Buddhist tattoo would I be disrespe...</td>\n",
       "      <td>Do many Buddhists actually \"throw away the raft\"?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "273872  273872  392405  392406   \n",
       "342308  342308  438846  462606   \n",
       "353135  353135  482139  482140   \n",
       "\n",
       "                                                question1  \\\n",
       "273872             What are the pros & cons of democracy?   \n",
       "342308  How will Brexit impact the flow of goods and p...   \n",
       "353135  If I got a Buddhist tattoo would I be disrespe...   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "273872         What are the pros and cons of a democracy?             1  \n",
       "342308  Can a post-Brexit Britain really survive witho...             1  \n",
       "353135  Do many Buddhists actually \"throw away the raft\"?             0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('./data/train.csv').sample(10000,random_state=44)\n",
    "df_test = pd.read_csv('./data/test.csv').sample(10000,random_state=44)\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of question pairs for training: 10000\n",
      "Total number of question pairs for test data: 10000\n",
      "Duplicate pairs : 37.46 %\n"
     ]
    }
   ],
   "source": [
    "print('Total number of question pairs for training: {}'.format(len(df_train)))\n",
    "print('Total number of question pairs for test data: {}'.format(len(df_test)))\n",
    "print('Duplicate pairs : {} %'.format(round(df_train['is_duplicate'].mean()*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text handcrafted features (fs_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_features(df_train):\n",
    "    df_train['len_q1'] = df_train['question1'].apply(lambda x:len(str(x)))\n",
    "    df_train['len_q2'] = df_train['question2'].apply(lambda x:len(str(x)))\n",
    "    df_train['diff_len'] = df_train.len_q1-df_train.len_q2\n",
    "    df_train['len_char_q1'] = df_train.question1.apply(lambda x:len(''.join(set(str(x).replace(' ','')))))\n",
    "    df_train['len_char_q2'] = df_train.question2.apply(lambda x:len(''.join(set(str(x).replace(' ','')))))\n",
    "    df_train['len_word_q1'] = df_train.question1.apply(lambda x:len(str(x).split()))\n",
    "    df_train['len_word_q2'] = df_train.question2.apply(lambda x:len(str(x).split()))\n",
    "    df_train['common_words'] = df_train.apply(lambda x:len(set(str(x['question1']).lower().split()).intersection(set(str(x['question2']).lower().split()))),axis=1)\n",
    "\n",
    "    df_train['fuzzy_qratio'] = df_train.apply(lambda x: fuzz.QRatio(str(x['question1']),str(x['question2'])),axis=1)\n",
    "    df_train['fuzzy_wratio'] = df_train.apply(lambda x:fuzz.WRatio(str(x['question1']),str(x['question2'])),axis=1)\n",
    "    df_train['fuzzy_partial_ratio'] = df_train.apply(lambda x:fuzz.partial_ratio(str(x['question1']),str(x['question2'])),axis=1)\n",
    "    return df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>len_q1</th>\n",
       "      <th>len_q2</th>\n",
       "      <th>diff_len</th>\n",
       "      <th>len_char_q1</th>\n",
       "      <th>len_char_q2</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>common_words</th>\n",
       "      <th>fuzzy_qratio</th>\n",
       "      <th>fuzzy_wratio</th>\n",
       "      <th>fuzzy_partial_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>273872</th>\n",
       "      <td>273872</td>\n",
       "      <td>392405</td>\n",
       "      <td>392406</td>\n",
       "      <td>What are the pros &amp; cons of democracy?</td>\n",
       "      <td>What are the pros and cons of a democracy?</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>42</td>\n",
       "      <td>-4</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>92</td>\n",
       "      <td>95</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342308</th>\n",
       "      <td>342308</td>\n",
       "      <td>438846</td>\n",
       "      <td>462606</td>\n",
       "      <td>How will Brexit impact the flow of goods and p...</td>\n",
       "      <td>Can a post-Brexit Britain really survive witho...</td>\n",
       "      <td>1</td>\n",
       "      <td>105</td>\n",
       "      <td>145</td>\n",
       "      <td>-40</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>62</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353135</th>\n",
       "      <td>353135</td>\n",
       "      <td>482139</td>\n",
       "      <td>482140</td>\n",
       "      <td>If I got a Buddhist tattoo would I be disrespe...</td>\n",
       "      <td>Do many Buddhists actually \"throw away the raft\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>49</td>\n",
       "      <td>36</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>86</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "273872  273872  392405  392406   \n",
       "342308  342308  438846  462606   \n",
       "353135  353135  482139  482140   \n",
       "\n",
       "                                                question1  \\\n",
       "273872             What are the pros & cons of democracy?   \n",
       "342308  How will Brexit impact the flow of goods and p...   \n",
       "353135  If I got a Buddhist tattoo would I be disrespe...   \n",
       "\n",
       "                                                question2  is_duplicate  \\\n",
       "273872         What are the pros and cons of a democracy?             1   \n",
       "342308  Can a post-Brexit Britain really survive witho...             1   \n",
       "353135  Do many Buddhists actually \"throw away the raft\"?             0   \n",
       "\n",
       "        len_q1  len_q2  diff_len  len_char_q1  len_char_q2  len_word_q1  \\\n",
       "273872      38      42        -4           17           16            8   \n",
       "342308     105     145       -40           26           30           18   \n",
       "353135      85      49        36           22           21           15   \n",
       "\n",
       "        len_word_q2  common_words  fuzzy_qratio  fuzzy_wratio  \\\n",
       "273872            9             7            92            95   \n",
       "342308           22             7            62            71   \n",
       "353135            8             1            44            86   \n",
       "\n",
       "        fuzzy_partial_ratio  \n",
       "273872                   87  \n",
       "342308                   71  \n",
       "353135                   49  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = generate_features(df_train)\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA (Lattent Dirichlet Allocation) features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Steaming \n",
    "p_stemmer = PorterStemmer()\n",
    "STOP_WORDS = nltk.corpus.stopwords.words()\n",
    "\n",
    "def clean_sentence(val):\n",
    "    \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n",
    "    regex = re.compile('([^\\s\\w]|_)+')\n",
    "    sentence = regex.sub('', val).lower()\n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    for word in list(sentence):\n",
    "        if word in STOP_WORDS:\n",
    "            sentence.remove(word)  \n",
    "            \n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence\n",
    "\n",
    "def clean_dataframe(data):\n",
    "    \"drop nans, then apply 'clean_sentence' function to question1 and 2\"\n",
    "    data = data.dropna(how=\"any\")\n",
    "    \n",
    "    for col in ['question1', 'question2']:\n",
    "        data[col] = data[col].apply(clean_sentence)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Function to vuild a corpus\n",
    "def build_corpus(data):\n",
    "    \"Creates a list of lists containing words from each sentence\"\n",
    "    corpus = []\n",
    "    for col in ['question1', 'question2']:\n",
    "        for sentence in data[col].iteritems():\n",
    "            word_list = sentence[1].split(\" \")\n",
    "            corpus.append(word_list)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = clean_dataframe(df_train)\n",
    "corpus = build_corpus(data)\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "corpus = [dictionary.doc2bow(text) for text in corpus]\n",
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=30, id2word = dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def common_lda_topic(sentence1,sentence2,dictionary,ldamodel,min_proba):\n",
    "    \"find #common topic based on lattent dirichlet allocation model\"\n",
    "    sentence1 = sentence1.split()\n",
    "    sentence2 = sentence2.split()\n",
    "\n",
    "    sentence1 = dictionary.doc2bow(sentence1)\n",
    "    sentence2 = dictionary.doc2bow(sentence2)\n",
    "    \n",
    "    topic_a = ldamodel.get_document_topics(sentence1,minimum_probability=min_proba)\n",
    "    topic_b = ldamodel.get_document_topics(sentence2,minimum_probability=min_proba)\n",
    "    \n",
    "    topic_a = list(sorted(topic_a, key=lambda x: x[1]))\n",
    "    topic_b = list(sorted(topic_b, key=lambda x: x[1]))\n",
    "    common_topic = set([x[0] for x in topic_a]).intersection(x[0] for x in topic_b)\n",
    "    return(len(common_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>len_q1</th>\n",
       "      <th>len_q2</th>\n",
       "      <th>diff_len</th>\n",
       "      <th>len_char_q1</th>\n",
       "      <th>len_char_q2</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>common_words</th>\n",
       "      <th>fuzzy_qratio</th>\n",
       "      <th>fuzzy_wratio</th>\n",
       "      <th>fuzzy_partial_ratio</th>\n",
       "      <th>common_topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>273872</th>\n",
       "      <td>273872</td>\n",
       "      <td>392405</td>\n",
       "      <td>392406</td>\n",
       "      <td>What are the pros &amp; cons of democracy?</td>\n",
       "      <td>What are the pros and cons of a democracy?</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>42</td>\n",
       "      <td>-4</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>92</td>\n",
       "      <td>95</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342308</th>\n",
       "      <td>342308</td>\n",
       "      <td>438846</td>\n",
       "      <td>462606</td>\n",
       "      <td>How will Brexit impact the flow of goods and p...</td>\n",
       "      <td>Can a post-Brexit Britain really survive witho...</td>\n",
       "      <td>1</td>\n",
       "      <td>105</td>\n",
       "      <td>145</td>\n",
       "      <td>-40</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>62</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353135</th>\n",
       "      <td>353135</td>\n",
       "      <td>482139</td>\n",
       "      <td>482140</td>\n",
       "      <td>If I got a Buddhist tattoo would I be disrespe...</td>\n",
       "      <td>Do many Buddhists actually \"throw away the raft\"?</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>49</td>\n",
       "      <td>36</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>86</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332098</th>\n",
       "      <td>332098</td>\n",
       "      <td>338155</td>\n",
       "      <td>31995</td>\n",
       "      <td>When did I create my Instagram account?</td>\n",
       "      <td>How can I track down who created an Instagram ...</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>54</td>\n",
       "      <td>-15</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176156</th>\n",
       "      <td>176156</td>\n",
       "      <td>271084</td>\n",
       "      <td>271085</td>\n",
       "      <td>Is Hulu Plus Free Trial really free?</td>\n",
       "      <td>How long is the Hulu Plus free trial?</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>-1</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>65</td>\n",
       "      <td>83</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "273872  273872  392405  392406   \n",
       "342308  342308  438846  462606   \n",
       "353135  353135  482139  482140   \n",
       "332098  332098  338155   31995   \n",
       "176156  176156  271084  271085   \n",
       "\n",
       "                                                question1  \\\n",
       "273872             What are the pros & cons of democracy?   \n",
       "342308  How will Brexit impact the flow of goods and p...   \n",
       "353135  If I got a Buddhist tattoo would I be disrespe...   \n",
       "332098            When did I create my Instagram account?   \n",
       "176156               Is Hulu Plus Free Trial really free?   \n",
       "\n",
       "                                                question2  is_duplicate  \\\n",
       "273872         What are the pros and cons of a democracy?             1   \n",
       "342308  Can a post-Brexit Britain really survive witho...             1   \n",
       "353135  Do many Buddhists actually \"throw away the raft\"?             0   \n",
       "332098  How can I track down who created an Instagram ...             0   \n",
       "176156              How long is the Hulu Plus free trial?             0   \n",
       "\n",
       "        len_q1  len_q2  diff_len  len_char_q1  len_char_q2  len_word_q1  \\\n",
       "273872      38      42        -4           17           16            8   \n",
       "342308     105     145       -40           26           30           18   \n",
       "353135      85      49        36           22           21           15   \n",
       "332098      39      54       -15           18           18            7   \n",
       "176156      36      37        -1           15           17            7   \n",
       "\n",
       "        len_word_q2  common_words  fuzzy_qratio  fuzzy_wratio  \\\n",
       "273872            9             7            92            95   \n",
       "342308           22             7            62            71   \n",
       "353135            8             1            44            86   \n",
       "332098           10             3            70            70   \n",
       "176156            8             4            65            83   \n",
       "\n",
       "        fuzzy_partial_ratio  common_topics  \n",
       "273872                   87              1  \n",
       "342308                   71              0  \n",
       "353135                   49              1  \n",
       "332098                   77              0  \n",
       "176156                   67              0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['common_topics'] = df_train.apply(lambda x:common_lda_topic(str(x['question1']),str(x['question2']),dictionary,ldamodel,0.1),axis=1)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS-Tagging features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def common_pos_tagging(question1,question2):\n",
    "    question1 = nltk.word_tokenize(question1)\n",
    "    question2 = nltk.word_tokenize(question2)\n",
    "    pos_question1 = nltk.pos_tag(question1)\n",
    "    pos_question2 = nltk.pos_tag(question2)\n",
    "\n",
    "    pos_1_array = [x[1] for x in pos_question1]\n",
    "    pos_2_array = [x[1] for x in pos_question2]\n",
    "    return(len(set(pos_1_array).intersection(pos_2_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xe2 in position 5: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-b3f2434f15ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'common_pos_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcommon_pos_tagging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[1;32m   3970\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3971\u001b[0m                         \u001b[0mreduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3972\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3973\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3974\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_apply_standard\u001b[0;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[1;32m   4062\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4063\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4064\u001b[0;31m                     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4065\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4066\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-b3f2434f15ae>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'common_pos_count'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcommon_pos_tagging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-ee0790f26b27>\u001b[0m in \u001b[0;36mcommon_pos_tagging\u001b[0;34m(question1, question2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcommon_pos_tagging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mquestion2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mquestion1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mquestion2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpos_question1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpos_question2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/tokenize/__init__.pyc\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m--> 106\u001b[0;31m     return [token for sent in sent_tokenize(text, language)\n\u001b[0m\u001b[1;32m    107\u001b[0m             for token in _treebank_word_tokenize(sent)]\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/tokenize/__init__.pyc\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \"\"\"\n\u001b[1;32m     90\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# Standard word tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m         \"\"\"\n\u001b[0;32m-> 1226\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36msentences_from_text\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \"\"\"\n\u001b[0;32m-> 1274\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36mspan_tokenize\u001b[0;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mslices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1265\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msentences_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[0;34m(self, text, slices)\u001b[0m\n\u001b[1;32m   1302\u001b[0m         \"\"\"\n\u001b[1;32m   1303\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m             \u001b[0msl1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \"\"\"\n\u001b[1;32m    309\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_slices_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1280\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1281\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'next_tok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36mtext_contains_sentbreak\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \"\"\"\n\u001b[1;32m   1324\u001b[0m         \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m \u001b[0;31m# used to ignore last token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1325\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_annotate_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_annotate_second_pass\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m   1458\u001b[0m         \u001b[0mheuristic\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4.1\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfrequent\u001b[0m \u001b[0msentence\u001b[0m \u001b[0mstarter\u001b[0m \u001b[0mheuristic\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m4.1\u001b[0m\u001b[0;36m.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m         \"\"\"\n\u001b[0;32m-> 1460\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1461\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_second_pass_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_pair_iter\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \"\"\"\n\u001b[1;32m    309\u001b[0m     \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m     \u001b[0mprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_annotate_first_pass\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    575\u001b[0m           \u001b[0;34m-\u001b[0m \u001b[0mellipsis_toks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mindices\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mellipsis\u001b[0m \u001b[0mmarks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \"\"\"\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0maug_tok\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_pass_annotation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_tok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0maug_tok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/nltk/tokenize/punkt.pyc\u001b[0m in \u001b[0;36m_tokenize_words\u001b[0;34m(self, plaintext)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0mparastart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mplaintext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m                 \u001b[0mline_toks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xe2 in position 5: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "df_train['common_pos_count'] = df_train.apply(lambda x:common_pos_tagging(str(x['question1']),str(x['question2'])),axis=1)\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ ******* SENTENCES TOPIC DISTRIBUTIONS ******* -------\n",
      "[(1, 0.24301468264363649), (21, 0.52365198402302937)]\n",
      "\n",
      "\n",
      "[(1, 0.24294080469988402), (21, 0.52372586196678184)]\n",
      "\n",
      "\n",
      "------ ******* WORD TOPIC DISTRIBUTIONS ******* -------\n",
      "topic : 21 ======> 0.029*\"help\" + 0.027*\"best\" + 0.024*\"movie\" + 0.023*\"free\" + 0.019*\"technology\" + 0.019*\"join\" + 0.018*\"salary\" + 0.015*\"indian\" + 0.014*\"fat\" + 0.013*\"engineer\" \n",
      "\n",
      "topic : 21 ======> 0.029*\"help\" + 0.027*\"best\" + 0.024*\"movie\" + 0.023*\"free\" + 0.019*\"technology\" + 0.019*\"join\" + 0.018*\"salary\" + 0.015*\"indian\" + 0.014*\"fat\" + 0.013*\"engineer\" \n",
      "\n",
      "------ ******* COMMON TOPICS ******* -------\n",
      "#common topic : set([1, 21]) ======> 2\n"
     ]
    }
   ],
   "source": [
    "query = 'How will Brexit impact the flow of goods and'\n",
    "query2 = 'How will Brexit impact the flow of goods and'\n",
    "query = query.split()\n",
    "query2 = query2.split()\n",
    "\n",
    "query = dictionary.doc2bow(query)\n",
    "query2 = dictionary.doc2bow(query2)\n",
    "\n",
    "topic_a = ldamodel.get_document_topics(query,minimum_probability=0.1)\n",
    "topic_b = ldamodel.get_document_topics(query2,minimum_probability=0.1)\n",
    "\n",
    "topic_a = list(sorted(topic_a, key=lambda x: x[1]))\n",
    "topic_b = list(sorted(topic_b, key=lambda x: x[1]))\n",
    "common_topic = set([x[0] for x in topic_a]).intersection(x[0] for x in topic_b)\n",
    "\n",
    "print(\"------ ******* SENTENCES TOPIC DISTRIBUTIONS ******* -------\")\n",
    "print(topic_a)\n",
    "print(\"\\n\")\n",
    "print(topic_b)\n",
    "print(\"\\n\")\n",
    "print(\"------ ******* WORD TOPIC DISTRIBUTIONS ******* -------\")\n",
    "print(\"topic : {} ======> {} \\n\".format(topic_a[-1][0],ldamodel.print_topic(topic_a[0][0])))\n",
    "print(\"topic : {} ======> {} \\n\".format(topic_b[-1][0],ldamodel.print_topic(topic_b[0][0])))\n",
    "print(\"------ ******* COMMON TOPICS ******* -------\")\n",
    "print(\"#common topic : {} ======> {}\".format(common_topic,len(common_topic)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Question for 1 is 12 : ['NNP', 'VBZ', 'DT', 'JJS', 'NN', 'IN', 'DT', 'NN', ',', 'PRP', 'VBZ', 'JJ']\n",
      "POS Question for 2 is 10 : ['WP', 'VBP', 'DT', 'NNS', 'CC', 'NNS', 'IN', 'DT', 'NN', '.']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "question1 = nltk.word_tokenize(\"Messi is the best player in the word, he is wonderful\")\n",
    "question2 = nltk.word_tokenize(\"What are the pros and cons of a democracy?\")\n",
    "pos_question1 = nltk.pos_tag(question1)\n",
    "pos_question2 = nltk.pos_tag(question2)\n",
    "\n",
    "pos_1_array = [x[1] for x in pos_question1]\n",
    "pos_2_array = [x[1] for x in pos_question2]\n",
    "\n",
    "print(\"POS Question for 1 is {} : {}\".format(len(pos_1_array),pos_1_array))\n",
    "print(\"POS Question for 2 is {} : {}\".format(len(pos_2_array),pos_2_array))\n",
    "\n",
    "print(len(set(pos_1_array).intersection(pos_2_array)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec features\n",
    "Generate word2vec based on pre-trained model on Google News corpus (3 billion running words) word vector model (3 million 300-dimension English word vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logreg = linear_model.LogisticRegression(C=1e5)\n",
    "X = df_train.ix[:, 6:,]\n",
    "Y = df_train.is_duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len_q1</th>\n",
       "      <th>len_q2</th>\n",
       "      <th>diff_len</th>\n",
       "      <th>len_char_q1</th>\n",
       "      <th>len_char_q2</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>common_words</th>\n",
       "      <th>fuzzy_qratio</th>\n",
       "      <th>fuzzy_wratio</th>\n",
       "      <th>fuzzy_partial_ratio</th>\n",
       "      <th>common_topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>273872</th>\n",
       "      <td>38</td>\n",
       "      <td>42</td>\n",
       "      <td>-4</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>92</td>\n",
       "      <td>95</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342308</th>\n",
       "      <td>105</td>\n",
       "      <td>145</td>\n",
       "      <td>-40</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>62</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353135</th>\n",
       "      <td>85</td>\n",
       "      <td>49</td>\n",
       "      <td>36</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>86</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332098</th>\n",
       "      <td>39</td>\n",
       "      <td>54</td>\n",
       "      <td>-15</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176156</th>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>-1</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>65</td>\n",
       "      <td>83</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        len_q1  len_q2  diff_len  len_char_q1  len_char_q2  len_word_q1  \\\n",
       "273872      38      42        -4           17           16            8   \n",
       "342308     105     145       -40           26           30           18   \n",
       "353135      85      49        36           22           21           15   \n",
       "332098      39      54       -15           18           18            7   \n",
       "176156      36      37        -1           15           17            7   \n",
       "\n",
       "        len_word_q2  common_words  fuzzy_qratio  fuzzy_wratio  \\\n",
       "273872            9             7            92            95   \n",
       "342308           22             7            62            71   \n",
       "353135            8             1            44            86   \n",
       "332098           10             3            70            70   \n",
       "176156            8             4            65            83   \n",
       "\n",
       "        fuzzy_partial_ratio  common_topics  \n",
       "273872                   87              1  \n",
       "342308                   71              0  \n",
       "353135                   49              1  \n",
       "332098                   77              1  \n",
       "176156                   67              0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100000.0, class_weight=None, dual=False,\n",
       "          fit_intercept=True, intercept_scaling=1, max_iter=100,\n",
       "          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = generate_features(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = df_train.ix[:, 6:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proba_replicated = logreg.predict_proba(test_data)\n",
    "proba = proba_replicated[:,1]\n",
    "len(proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>len_q1</th>\n",
       "      <th>len_q2</th>\n",
       "      <th>diff_len</th>\n",
       "      <th>len_char_q1</th>\n",
       "      <th>len_char_q2</th>\n",
       "      <th>len_word_q1</th>\n",
       "      <th>len_word_q2</th>\n",
       "      <th>common_words</th>\n",
       "      <th>fuzzy_qratio</th>\n",
       "      <th>fuzzy_wratio</th>\n",
       "      <th>fuzzy_partial_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>273872</th>\n",
       "      <td>273872</td>\n",
       "      <td>392405</td>\n",
       "      <td>392406</td>\n",
       "      <td>pros  cons democracy</td>\n",
       "      <td>pros cons democracy</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>42</td>\n",
       "      <td>-4</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>92</td>\n",
       "      <td>95</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342308</th>\n",
       "      <td>342308</td>\n",
       "      <td>438846</td>\n",
       "      <td>462606</td>\n",
       "      <td>brexit impact flow goods people northern irela...</td>\n",
       "      <td>postbrexit britain really survive without land...</td>\n",
       "      <td>1</td>\n",
       "      <td>105</td>\n",
       "      <td>145</td>\n",
       "      <td>-40</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>62</td>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353135</th>\n",
       "      <td>353135</td>\n",
       "      <td>482139</td>\n",
       "      <td>482140</td>\n",
       "      <td>got buddhist tattoo would disrespecting people...</td>\n",
       "      <td>many buddhists actually throw away raft</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>49</td>\n",
       "      <td>36</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>86</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332098</th>\n",
       "      <td>332098</td>\n",
       "      <td>338155</td>\n",
       "      <td>31995</td>\n",
       "      <td>create instagram account</td>\n",
       "      <td>track created instagram account</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>54</td>\n",
       "      <td>-15</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176156</th>\n",
       "      <td>176156</td>\n",
       "      <td>271084</td>\n",
       "      <td>271085</td>\n",
       "      <td>hulu plus free trial really free</td>\n",
       "      <td>long hulu plus free trial</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>-1</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>65</td>\n",
       "      <td>83</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "273872  273872  392405  392406   \n",
       "342308  342308  438846  462606   \n",
       "353135  353135  482139  482140   \n",
       "332098  332098  338155   31995   \n",
       "176156  176156  271084  271085   \n",
       "\n",
       "                                                question1  \\\n",
       "273872                               pros  cons democracy   \n",
       "342308  brexit impact flow goods people northern irela...   \n",
       "353135  got buddhist tattoo would disrespecting people...   \n",
       "332098                           create instagram account   \n",
       "176156                   hulu plus free trial really free   \n",
       "\n",
       "                                                question2  is_duplicate  \\\n",
       "273872                                pros cons democracy             1   \n",
       "342308  postbrexit britain really survive without land...             1   \n",
       "353135            many buddhists actually throw away raft             0   \n",
       "332098                    track created instagram account             0   \n",
       "176156                          long hulu plus free trial             0   \n",
       "\n",
       "        len_q1  len_q2  diff_len  len_char_q1  len_char_q2  len_word_q1  \\\n",
       "273872      38      42        -4           17           16            8   \n",
       "342308     105     145       -40           26           30           18   \n",
       "353135      85      49        36           22           21           15   \n",
       "332098      39      54       -15           18           18            7   \n",
       "176156      36      37        -1           15           17            7   \n",
       "\n",
       "        len_word_q2  common_words  fuzzy_qratio  fuzzy_wratio  \\\n",
       "273872            9             7            92            95   \n",
       "342308           22             7            62            71   \n",
       "353135            8             1            44            86   \n",
       "332098           10             3            70            70   \n",
       "176156            8             4            65            83   \n",
       "\n",
       "        fuzzy_partial_ratio  \n",
       "273872                   87  \n",
       "342308                   71  \n",
       "353135                   49  \n",
       "332098                   77  \n",
       "176156                   67  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOP_WORDS = nltk.corpus.stopwords.words()\n",
    "def clean_sentence(val):\n",
    "    \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n",
    "    regex = re.compile('([^\\s\\w]|_)+')\n",
    "    sentence = regex.sub('', val).lower()\n",
    "    sentence = sentence.split(\" \")\n",
    "    \n",
    "    for word in list(sentence):\n",
    "        if word in STOP_WORDS:\n",
    "            sentence.remove(word)  \n",
    "            \n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence\n",
    "\n",
    "def clean_dataframe(data):\n",
    "    \"drop nans, then apply 'clean_sentence' function to question1 and 2\"\n",
    "    data = data.dropna(how=\"any\")\n",
    "    \n",
    "    for col in ['question1', 'question2']:\n",
    "        data[col] = data[col].apply(clean_sentence)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = clean_dataframe(df_train)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.word_tokenize(\"And now for something completely different\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
